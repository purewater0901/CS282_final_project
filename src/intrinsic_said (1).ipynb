{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11054206,
          "sourceType": "datasetVersion",
          "datasetId": 6881580
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "TL2146N7AyVw"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL. vb\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "katsuyamucb_madde_dataset_path = kagglehub.dataset_download('katsuyamucb/madde-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "yVAzoS1tAyVy"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ls /root/.cache/kagglehub/datasets/katsuyamucb/madde-dataset/versions/13\n"
      ],
      "metadata": {
        "id": "xPSasJnKC1br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Codebase\n"
      ],
      "metadata": {
        "id": "MAfPAZvfAyVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our existing codebase for fine tuning visual models. The goal of this project is to find best PEFT methods for deepfake detection.\n",
        "We tried a couple of 'deepfake detection' models below, and (but their performance is not great at all.\n",
        "So we use our deepfake dataset (~10K) to fine-tune the pre-trained deepfake detection model, and let them work better.\n",
        "\n",
        "What we want to spend time on are;\n",
        "  - Try as many PEFT methods as possible\n",
        "  - Record their performance in Weights and Biases\n",
        "  - Think of why some work better and some don't\n",
        "  - Compare them to full fine tuning\n",
        "  - Refactor our codebase\n",
        "\n",
        "However\n",
        "  - We don't try too many models for simplicity; Still not 100% but let's **use pre-trained CLIP** for testing.\n",
        "  - We don't work on non-facial forgery. Let's focus on **forged facial image**.\n"
      ],
      "metadata": {
        "id": "G9imDjzlAyVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Setup\n",
        "\n",
        "In the setup section, we will define three utility functions;\n",
        "- DeepfakeDataset(): Use selected folder names to generate dataset\n",
        "- FineTuner: A Class to inherit - it has basic functions such as model loading, data processing, and model evaluation.\n",
        "- FullFT: A subclass of FineTuner to run full fine tuning. Please refer this to record training/validation logs using wandb"
      ],
      "metadata": {
        "id": "rmQEQJAaAyVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Libraries\n",
        "import os\n",
        "import torch\n",
        "import timm\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm  # Import tqdm for progress bars\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Optional, Union, Tuple\n",
        "from IPython.display import display\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Wandb\n",
        "import wandb\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T18:49:03.506977Z",
          "iopub.execute_input": "2025-04-05T18:49:03.507289Z",
          "iopub.status.idle": "2025-04-05T18:49:03.513568Z",
          "shell.execute_reply.started": "2025-04-05T18:49:03.507265Z",
          "shell.execute_reply": "2025-04-05T18:49:03.512477Z"
        },
        "id": "2U_lyDH-AyVz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/purewater0901/deepfake-detection.git\n",
        "\n",
        "from deepfake_detection.model.dfdet import DeepfakeDetectionModel\n",
        "from deepfake_detection.config import Config"
      ],
      "metadata": {
        "id": "99WKjV11fMpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login with your wandb API keys!"
      ],
      "metadata": {
        "id": "MMd9UHv8AyVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please use your own login API key\n",
        "! wandb login"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T18:49:05.2982Z",
          "iopub.execute_input": "2025-04-05T18:49:05.298483Z",
          "iopub.status.idle": "2025-04-05T18:49:07.537002Z",
          "shell.execute_reply.started": "2025-04-05T18:49:05.298462Z",
          "shell.execute_reply": "2025-04-05T18:49:07.536118Z"
        },
        "id": "tphRak1nAyV0",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "FvKfmBYYAyV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset for deepfake detection\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Custom dataset for testing deepfake detection models with customizable class folders\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir: str,\n",
        "        real_folder: str = 'Real',\n",
        "        fake_folder: str = 'Fake',\n",
        "        transform=None,\n",
        "        processor=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Root directory containing class folders\n",
        "            real_folder (str): Name of the folder containing real images\n",
        "            fake_folder (str): Name of the folder containing fake images\n",
        "            transform (callable, optional): Optional transform to be applied on images\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.processor = processor\n",
        "        self.class_folders = {\n",
        "            0: real_folder,  # 0 = real\n",
        "            1: fake_folder,  # 1 = fake\n",
        "        }\n",
        "\n",
        "        self.samples = []\n",
        "        self.load_samples()\n",
        "\n",
        "    def load_samples(self):\n",
        "        \"\"\"Load all image paths and their corresponding labels\"\"\"\n",
        "        for class_idx, folder_name in self.class_folders.items():\n",
        "            class_dir = os.path.join(self.root_dir, folder_name)\n",
        "            if not os.path.exists(class_dir):\n",
        "                raise FileNotFoundError(f\"Directory not found: {class_dir}\")\n",
        "\n",
        "            # Add all valid images from this class folder\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                if img_name.lower().endswith(('.png')): # We restrict png format, in order to avoid overfitting to the difference in format\n",
        "                    img_path = os.path.join(class_dir, img_name)\n",
        "                    self.samples.append((img_path, class_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            elif self.processor:\n",
        "                image = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
        "                # processor returns dictionary, so reduce dimension here\n",
        "\n",
        "            return image, label, img_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a placeholder image and the label\n",
        "            placeholder = torch.zeros((3, 299, 299))\n",
        "            return placeholder, label, img_path\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T18:49:07.538638Z",
          "iopub.execute_input": "2025-04-05T18:49:07.539011Z",
          "iopub.status.idle": "2025-04-05T18:49:07.547513Z",
          "shell.execute_reply.started": "2025-04-05T18:49:07.538972Z",
          "shell.execute_reply": "2025-04-05T18:49:07.546896Z"
        },
        "id": "XuM_jhx7AyV0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for full fine tuning.\n",
        "# Please inherit this as a super class when you develop a new finetuner.\n",
        "\n",
        "class FineTuner():\n",
        "    \"\"\"\n",
        "    A Class of fine-tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb = False, model = None, processor = None):\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Load the model using timm if the model is None\n",
        "        if model == None:\n",
        "            self.model = timm.create_model(self.model_name, pretrained=True, num_classes=2)\n",
        "            print(\"Loaded \", model_name, \" for fine-tuning\")\n",
        "        else:\n",
        "            self.model = model\n",
        "        # print(self.model)\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.real_folder = real_folder\n",
        "        self.fake_folder = fake_folder\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.use_wandb = use_wandb\n",
        "        self.test_fake_folder = None\n",
        "        self.processor = processor\n",
        "\n",
        "\n",
        "    def set_TestFolder(self, test_fake_folder):\n",
        "        self.test_fake_folder = test_fake_folder\n",
        "\n",
        "    def get_Train_Val_loader(self):\n",
        "\n",
        "        if self.processor:\n",
        "            train_dataset = DeepfakeDataset(\n",
        "                root_dir=self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Train'),\n",
        "                fake_folder= os.path.join(self.fake_folder, 'Train'),\n",
        "                processor = self.processor\n",
        "            )\n",
        "\n",
        "            val_dataset = DeepfakeDataset(\n",
        "                root_dir= self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Validation'),\n",
        "                fake_folder= os.path.join(self.fake_folder, 'Validation'),\n",
        "                processor = self.processor\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Get the config file from timm\n",
        "            config = resolve_data_config({}, model=self.model)\n",
        "            base_transform = create_transform(**config)\n",
        "            # Data augmentation and normalization for training\n",
        "            train_transform_list = base_transform.transforms\n",
        "            train_transform_list.append(transforms.RandomHorizontalFlip())\n",
        "            train_transform_list.append(transforms.RandomRotation(10))\n",
        "\n",
        "            brightness = np.random.uniform(0.05, 0.2)  # Random value between 0.05 and 0.2... you can change if you want\n",
        "            contrast = np.random.uniform(0.05, 0.2)\n",
        "            saturation = np.random.uniform(0.05, 0.2)\n",
        "            hue = np.random.uniform(0, 0.1)  # Hue is typically smaller values\n",
        "            train_transform_list.append(transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue = hue))\n",
        "\n",
        "            train_transform = transforms.Compose(train_transform_list)\n",
        "            val_transform = base_transform\n",
        "\n",
        "            # Create datasets\n",
        "            train_dataset = DeepfakeDataset(\n",
        "                root_dir=self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Train'),\n",
        "                fake_folder= os.path.join(self.fake_folder, 'Train'),\n",
        "                transform=train_transform\n",
        "            )\n",
        "\n",
        "            val_dataset = DeepfakeDataset(\n",
        "                root_dir= self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Validation'),\n",
        "                fake_folder= os.path.join(self.fake_folder, 'Validation'),\n",
        "                transform=val_transform,\n",
        "            )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def get_Test_loader(self, test_folder):\n",
        "        target = test_folder\n",
        "        if self.processor:\n",
        "            # Create dataset\n",
        "            dataset = DeepfakeDataset(\n",
        "                root_dir=self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Test'),\n",
        "                fake_folder= os.path.join(target, 'Test'),\n",
        "                processor = self.processor\n",
        "            )\n",
        "        else:\n",
        "            # Get the config file from timm\n",
        "            config = resolve_data_config({}, model=self.model)\n",
        "            base_transform = create_transform(**config)\n",
        "\n",
        "\n",
        "            # Create dataset\n",
        "            dataset = DeepfakeDataset(\n",
        "                root_dir=self.data_dir,\n",
        "                real_folder= os.path.join(self.real_folder, 'Test'),\n",
        "                fake_folder= os.path.join(target, 'Test'),\n",
        "                transform=base_transform\n",
        "            )\n",
        "\n",
        "        # Create data loader\n",
        "        data_loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=4,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    def Tune(self):\n",
        "        # function to override\n",
        "        pass\n",
        "\n",
        "\n",
        "    def Evaluation(self, test_folder, model = None):\n",
        "\n",
        "        # Set device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Get dataloader\n",
        "        test_loader = self.get_Test_loader(test_folder)\n",
        "        print('\\n\\n----- Test on ',test_folder,'-----')\n",
        "        print(f\"Device: {device}\")\n",
        "\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        if model:\n",
        "            self.model = model\n",
        "        self.model.eval()\n",
        "        model = self.model.to(device)\n",
        "\n",
        "        # Lists to store results\n",
        "        all_preds = []\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "        all_paths = []\n",
        "        confidence_threshold = 0.5\n",
        "\n",
        "        # Run inference\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, paths in tqdm(test_loader, desc=\"Testing\"):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs).logits_labels\n",
        "\n",
        "                # Convert outputs to probabilities\n",
        "                probs = torch.softmax(outputs.float(), dim=1).cpu().numpy() # BF16 to float()\n",
        "\n",
        "                # Convert to binary predictions using threshold\n",
        "                preds = (probs[:,1] >= confidence_threshold).astype(int)\n",
        "\n",
        "                # Store results\n",
        "                all_preds.extend(preds)\n",
        "                all_probs.extend(probs)\n",
        "                all_labels.extend(labels.numpy())\n",
        "                all_paths.extend(paths)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_probs = np.array(all_probs)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        # Classification report\n",
        "        report = classification_report(all_labels, all_preds,\n",
        "                                      target_names=['Real', 'Fake'],\n",
        "                                      output_dict=True)\n",
        "\n",
        "        test_acc = (all_preds == all_labels).mean()\n",
        "\n",
        "        print(f\"\\n\\n{'-'*50}\")\n",
        "        print(f\"Test Result Summary:\")\n",
        "        print(f\"{'-'*50}\")\n",
        "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "        # Visualize the result ------------------------------------------------\n",
        "        report_df = pd.DataFrame(report)\n",
        "        display(report_df)\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(2)\n",
        "        plt.xticks(tick_marks, ['Real', 'Fake'], rotation=45)\n",
        "        plt.yticks(tick_marks, ['Real', 'Fake'])\n",
        "\n",
        "        # Add text annotations to confusion matrix\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                        horizontalalignment=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Logging\n",
        "        if self.use_wandb:\n",
        "\n",
        "            all_probs = np.array(all_probs)\n",
        "            fpr, tpr, _ = roc_curve(all_labels, all_probs[:,1])\n",
        "            wandb.log({\n",
        "                \"test_dataset\": test_folder,\n",
        "                \"test_accuracy\": test_acc,\n",
        "                # Confusion Matrix\n",
        "                \"test_confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                    probs=None,\n",
        "                    y_true=all_labels,\n",
        "                    preds=all_preds,\n",
        "                    class_names=['Real', 'Fake']\n",
        "                ),\n",
        "                # ROC curve\n",
        "                \"test_roc_curve\": wandb.plot.roc_curve(\n",
        "                    y_true= all_labels,\n",
        "                    y_probas= all_probs,\n",
        "                    labels=['Real', 'Fake']\n",
        "                )\n",
        "            })\n",
        "\n",
        "        return report_df\n",
        "\n",
        "    def log_wandb_train(\n",
        "            self,\n",
        "            all_labels,\n",
        "            all_preds,\n",
        "            all_probs,\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            train_acc,\n",
        "            val_loss,\n",
        "            val_acc,\n",
        "            optimizer,\n",
        "        ):\n",
        "        if self.use_wandb:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_accuracy\": val_acc,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            # Extra logging for the last epoch\n",
        "            if epoch == self.num_epochs - 1 and self.use_wandb:\n",
        "                # Confusion Matrix\n",
        "                cm = confusion_matrix(all_labels, all_preds)\n",
        "                wandb.log({\n",
        "                    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
        "                        probs=None,\n",
        "                        y_true=all_labels,\n",
        "                        preds=all_preds,\n",
        "                        class_names=['Real', 'Fake']\n",
        "                    )\n",
        "                })\n",
        "\n",
        "                # ROC curve - use probs instead of preds\n",
        "                all_probs = np.array(all_probs)\n",
        "                fpr, tpr, _ = roc_curve(all_labels, all_probs[:,1])\n",
        "                wandb.log({\n",
        "                    \"roc_curve\": wandb.plot.roc_curve(\n",
        "                        y_true= all_labels,\n",
        "                        y_probas= all_probs,\n",
        "                        labels=['Real', 'Fake']\n",
        "                    )\n",
        "                })\n",
        "\n",
        "    def Experiment(self, wandb_run_name):\n",
        "        # Initilize Wandb\n",
        "        if self.use_wandb:\n",
        "            wandb.init(project='Fine-Tuning Experiment', name=wandb_run_name)\n",
        "\n",
        "            # Log the experimental setting\n",
        "            wandb.config.update({\n",
        "                \"model\": self.model_name,\n",
        "                \"batch_size\": self.batch_size,\n",
        "                \"learning_rate\": self.learning_rate,\n",
        "                \"num_epochs\": self.num_epochs,\n",
        "                \"fine_tuning_type\": \"full\",\n",
        "                \"dataset_dir\": self.data_dir,\n",
        "                \"real_folder\": self.real_folder,\n",
        "                \"fake_folder\": self.fake_folder\n",
        "            })\n",
        "\n",
        "            total_params = sum(p.numel() for p in self.model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "            wandb.log({\n",
        "                \"total_parameters\": total_params,\n",
        "                \"trainable_parameters\": trainable_params,\n",
        "                \"frozen_parameters\": total_params - trainable_params,\n",
        "                \"percent_trainable\": 100 * trainable_params / total_params\n",
        "            })\n",
        "\n",
        "            # Log the model itself, (don't know if we need this or not)\n",
        "            # wandb.watch(self.model, log=\"all\", log_freq=100)\n",
        "\n",
        "        # Fine-tune the model\n",
        "        tuned_model = self.Tune()\n",
        "        report_df_seen = self.Evaluation(self.fake_folder)\n",
        "        if self.test_fake_folder != None:\n",
        "            report_df_unseen = self.Evaluation(self.test_fake_folder)\n",
        "\n",
        "        if self.use_wandb:\n",
        "            model_artifact = wandb.Artifact(\n",
        "                name=f\"{self.method_name}-{self.model_name}\",\n",
        "                type=\"model\"\n",
        "            )\n",
        "            #model_artifact.add_file(model_save_path)\n",
        "            wandb.log_artifact(model_artifact)\n",
        "            wandb.finish()\n",
        "\n",
        "        return tuned_model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-05T18:49:07.548808Z",
          "iopub.execute_input": "2025-04-05T18:49:07.549045Z",
          "iopub.status.idle": "2025-04-05T18:49:07.577558Z",
          "shell.execute_reply.started": "2025-04-05T18:49:07.549025Z",
          "shell.execute_reply": "2025-04-05T18:49:07.576654Z"
        },
        "id": "dE1Ahfm0AyV0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Hope fully we run this too and compare it with PEFT, but full fine tuning with 10+ epochs takes very long time.\n",
        "# To see how the training works, please set the epoch to <5.\n",
        "\n",
        "class FullFT(FineTuner):\n",
        "    def __init__(self, model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb= False, model = None, processor = None):\n",
        "        super().__init__(model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb, model, processor)\n",
        "        self.method_name = 'Full_FT'\n",
        "\n",
        "    def Tune(self):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory containing 'train' and 'val' subdirectories,\n",
        "                            each with 'real' and 'fake' subdirectories\n",
        "            real_folder:\n",
        "            fake_folder:\n",
        "            num_epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            learning_rate (float): Learning rate for optimizer\n",
        "            use_wandb (boolean): Use wandb's logging\n",
        "        \"\"\"\n",
        "\n",
        "        # Set device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        # Load the model\n",
        "        #model = timm.create_model(self.model_name, pretrained=True, num_classes=2)\n",
        "        self.model = self.model.to(device)\n",
        "        model_save_path= f\"{self.model_name}.pth\"\n",
        "\n",
        "        # Get data loader for training and validation\n",
        "        train_loader, val_loader = self.get_Train_Val_loader()\n",
        "\n",
        "        # Loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        # Create a tqdm progress bar for epochs\n",
        "        epoch_loop = tqdm(range(self.num_epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
        "        for epoch in epoch_loop:\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            for inputs, labels, pathes in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(inputs).float()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "            # Calculate epoch metrics\n",
        "            train_loss = running_loss / len(train_loader.dataset)\n",
        "            train_acc = correct / total\n",
        "            train_losses.append(train_loss)\n",
        "            train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_running_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            all_probs = []\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels, pathes in val_loader: #val_loop:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    val_running_loss += loss.item() * inputs.size(0)\n",
        "                    probs = torch.nn.functional.softmax(outputs.float(), dim=1)\n",
        "                    _, predicted = torch.max(outputs.float(), 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            val_loss = val_running_loss / len(val_loader.dataset)\n",
        "            val_acc = val_correct / val_total\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_acc)\n",
        "\n",
        "            # Learning rate scheduler step\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            print(f\"\\n\\n{'-'*50}\")\n",
        "            print(f\"Epoch {epoch+1}/{self.num_epochs} Summary:\")\n",
        "            print(f\"{'-'*50}\")\n",
        "            # Save the best model\n",
        "            if val_acc >= best_val_acc:\n",
        "                torch.save(self.model.state_dict(), model_save_path)\n",
        "                print(f\"âœ… New best model saved! Validation accuracy: {val_acc:.4f} (previous best: {best_val_acc:.4f})\")\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "            # Also save a checkpoint every 5 epoch\n",
        "            if epoch + 1 % 5 == 0:\n",
        "                checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'train_loss': train_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                    'train_acc': train_acc,\n",
        "                    'val_acc': val_acc,\n",
        "                    'best_val_acc': best_val_acc\n",
        "                }, checkpoint_path)\n",
        "                print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "            print(f\"Training:   Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f} ({correct}/{total})\")\n",
        "            print(f\"Validation: Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} ({val_correct}/{val_total})\")\n",
        "\n",
        "            # Calculate and display improvement or regression\n",
        "            if epoch > 0:\n",
        "                train_loss_change = train_loss - train_losses[-2]\n",
        "                train_acc_change = train_acc - train_accuracies[-2]\n",
        "                val_loss_change = val_loss - val_losses[-2]\n",
        "                val_acc_change = val_acc - val_accuracies[-2]\n",
        "\n",
        "                print(f\"Changes from previous epoch:\")\n",
        "                print(f\"  Train Loss: {train_loss_change:+.4f} | Train Acc: {train_acc_change:+.4f}\")\n",
        "                print(f\"  Val Loss: {val_loss_change:+.4f} | Val Acc: {val_acc_change:+.4f}\")\n",
        "\n",
        "            # Display current learning rate\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Current learning rate: {current_lr:.6f}\")\n",
        "\n",
        "            #Load the best model\n",
        "            print(f\"Loading best model from {model_save_path}\")\n",
        "            self.model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "            # Log in wandb\n",
        "            self.log_wandb_train(\n",
        "                all_labels,\n",
        "                all_preds,\n",
        "                all_probs,\n",
        "                epoch,\n",
        "                train_loss,\n",
        "                train_acc,\n",
        "                val_loss,\n",
        "                val_acc,\n",
        "                optimizer\n",
        "            )\n",
        "\n",
        "        # Final summary at the end of training\n",
        "        tqdm.write(f\"\\nTraining completed after {self.num_epochs} epochs\")\n",
        "        tqdm.write(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "pC6hIN39w0x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intrinsic SAID"
      ],
      "metadata": {
        "id": "RPJ0VF9Q97Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "5r02xj86rf85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja\n"
      ],
      "metadata": {
        "id": "KcvlerBvqg6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "wht = load(\n",
        "    name=\"wht\",\n",
        "    sources=[\"fwh_cpp.cpp\", \"fwh_cu.cu\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "DhWgAKnbruQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wht)"
      ],
      "metadata": {
        "id": "9PZTQfUds0g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The codes are from Armen Aghajanyan from facebook, from paper\n",
        "# Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\n",
        "# https://arxiv.org/abs/2012.13255\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Tuple, Set\n",
        "from wht import fast_walsh_hadamard_transform as fast_walsh_hadamard_transform_cuda\n",
        "\n",
        "\n",
        "def fast_walsh_hadamard_torched(x, axis: int = 0, normalize: bool = True):\n",
        "    orig_shape = x.size()\n",
        "    assert axis >= 0 and axis < len(orig_shape), (\n",
        "        \"For a vector of shape %s, axis must be in [0, %d] but it is %d\"\n",
        "        % (orig_shape, len(orig_shape) - 1, axis)\n",
        "    )\n",
        "    h_dim = orig_shape[axis]\n",
        "    h_dim_exp = int(round(np.log(h_dim) / np.log(2)))\n",
        "    assert h_dim == 2 ** h_dim_exp, (\n",
        "        \"hadamard can only be computed over axis with size that is a power of two, but\"\n",
        "        \" chosen axis %d has size %d\" % (axis, h_dim)\n",
        "    )\n",
        "\n",
        "    working_shape_pre = [int(torch.prod(torch.tensor(orig_shape[:axis])))]\n",
        "    working_shape_post = [\n",
        "        int(torch.prod(torch.tensor(orig_shape[axis + 1:])))\n",
        "    ]\n",
        "    working_shape_mid = [2] * h_dim_exp\n",
        "    working_shape = working_shape_pre + working_shape_mid + working_shape_post\n",
        "\n",
        "    ret = x.view(working_shape)\n",
        "\n",
        "    for ii in range(h_dim_exp):\n",
        "        dim = ii + 1\n",
        "        arrs = torch.chunk(ret, 2, dim=dim)\n",
        "        assert len(arrs) == 2\n",
        "        ret = torch.cat((arrs[0] + arrs[1], arrs[0] - arrs[1]), axis=dim)\n",
        "\n",
        "    if normalize:\n",
        "        ret = ret / np.sqrt(float(h_dim))\n",
        "\n",
        "    ret = ret.view(orig_shape)\n",
        "\n",
        "    return ret\n",
        "\n",
        "\n",
        "def fastfood_vars(DD, device=0):\n",
        "    \"\"\"\n",
        "    Returns parameters for fast food transform\n",
        "    :param DD: desired dimension\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    ll = int(np.ceil(np.log(DD) / np.log(2)))\n",
        "    LL = 2 ** ll\n",
        "\n",
        "    # Binary scaling matrix where $B_{i,i} \\in \\{\\pm 1 \\}$ drawn iid\n",
        "    BB = torch.FloatTensor(LL).uniform_(0, 2).type(torch.LongTensor)\n",
        "    BB = (BB * 2 - 1)\n",
        "    BB.requires_grad_(False)\n",
        "\n",
        "    # Random permutation matrix\n",
        "    Pi = torch.LongTensor(np.random.permutation(LL))\n",
        "    Pi.requires_grad_(False)\n",
        "\n",
        "    # Gaussian scaling matrix, whose elements $G_{i,i} \\sim \\mathcal{N}(0, 1)$\n",
        "    GG = torch.FloatTensor(LL,).normal_()\n",
        "    GG.requires_grad_(False)\n",
        "    divisor = torch.sqrt(LL * torch.sum(torch.pow(GG, 2)))\n",
        "    return [BB.to(device), Pi.to(device), GG.to(device), divisor.to(device), LL]\n",
        "\n",
        "\n",
        "def random_vars(desired_dim, intrinsic_dim, device=0):\n",
        "    \"\"\"Returns a random matrix of the desired dimension.\"\"\"\n",
        "    R = torch.FloatTensor(desired_dim, intrinsic_dim).normal_(std=0.01).to(device)\n",
        "    R.requires_grad_(False)\n",
        "    divisor = torch.norm(R)\n",
        "    return [R, divisor]\n",
        "\n",
        "\n",
        "def fastfood_torched(x, DD: int, param_list: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int]):\n",
        "    \"\"\"\n",
        "    Fastfood transform\n",
        "    :param x: array of dd dimension\n",
        "    :param DD: desired dimension\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    dd = x.size(0)\n",
        "\n",
        "    BB, Pi, GG, divisor, LL = param_list\n",
        "    # Padd x if needed\n",
        "    dd_pad = F.pad(x, pad=(0, LL - dd), value=0.0, mode=\"constant\")\n",
        "    # From left to right HGPiH(BX), where H is Walsh-Hadamard matrix\n",
        "    dd_pad = dd_pad * BB\n",
        "\n",
        "    # HGPi(HBX)\n",
        "    mul_2 = FastWalshHadamard.apply(dd_pad)\n",
        "\n",
        "    # HG(PiHBX)\n",
        "    mul_3 = mul_2[Pi]\n",
        "\n",
        "    # H(GPiHBX)\n",
        "    mul_3 = mul_3 * GG\n",
        "\n",
        "    # (HGPiHBX)\n",
        "    mul_5 = FastWalshHadamard.apply(mul_3)\n",
        "\n",
        "    ret = mul_5[:int(DD)]\n",
        "    ret = ret / \\\n",
        "        (divisor * np.sqrt(float(DD) / LL))\n",
        "    return ret\n",
        "\n",
        "\n",
        "def random_torched(intrinsic_vec, param_list: Tuple[torch.Tensor, int]):\n",
        "    \"\"\"Random dense transform\"\"\"\n",
        "    R, divisor = param_list\n",
        "    result = torch.matmul(R, intrinsic_vec)\n",
        "    # TODO: for now we are not normalizing with the divisor, to be added later.\n",
        "    return result\n",
        "\n",
        "\n",
        "class FastWalshHadamard(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(torch.tensor(\n",
        "            [1 / np.sqrt(float(input.size(0)))]).to(input))\n",
        "        if input.is_cuda:\n",
        "            return fast_walsh_hadamard_transform_cuda(input.float(), False)\n",
        "        else:\n",
        "            return fast_walsh_hadamard_torched(input.float(), normalize=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        if grad_output.is_cuda:\n",
        "            return input*fast_walsh_hadamard_transform_cuda(grad_output.clone().float(), False).to(grad_output)\n",
        "        else:\n",
        "            return input*fast_walsh_hadamard_torched(grad_output.clone().float(), normalize=False).to(grad_output)\n",
        "\n",
        "\n",
        "class IntrinsicDimensionLight:\n",
        "    def __init__(self, module: nn.Module, intrinsic_dimension: int,  output_dir,\n",
        "                 str_filter: Set[str] = set(), said=False, projection=\"fastfood\", device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Adds hook only for the parameters selected inside the str_filter, and if str_filter is empty, this selects\n",
        "        all the parameters with gradient = True.\n",
        "        \"\"\"\n",
        "        self.projection = projection\n",
        "        self.name_base_localname = []\n",
        "        self.initial_value = dict()\n",
        "        self.projection_params = {}\n",
        "        self.said = said\n",
        "        self.device = device\n",
        "        self.said_size = len(list(module.named_parameters()))\n",
        "        if self.said:\n",
        "            print(f\"Intrinsic Dimension: {intrinsic_dimension}\")\n",
        "            print(f\"SAID Size: {self.said_size}\")\n",
        "            assert intrinsic_dimension > self.said_size\n",
        "            intrinsic_dimension -= (self.said_size+1)\n",
        "\n",
        "        self.intrinsic_dimension = intrinsic_dimension\n",
        "        self.intrinsic_parameter = nn.Parameter(\n",
        "            torch.zeros((intrinsic_dimension)).cpu() if device==\"cpu\" else torch.zeros((intrinsic_dimension)).cuda())\n",
        "        module.register_parameter(\n",
        "            \"intrinsic_parameter\", self.intrinsic_parameter)\n",
        "        setattr(module, \"intrinsic_parameter\", self.intrinsic_parameter)\n",
        "\n",
        "        length = 0\n",
        "        for name, param in module.named_parameters():\n",
        "            if param.requires_grad and (len(str_filter) == 0 or any([x in name for x in str_filter])):\n",
        "                length += 1\n",
        "                self.initial_value[name] = v0 = (\n",
        "                    param.clone().detach().requires_grad_(False).to(self.intrinsic_parameter.device)\n",
        "                )\n",
        "                DD = np.prod(v0.size())\n",
        "                self.projection_params[name] = self.get_projection_params(DD, self.intrinsic_parameter.device)\n",
        "                base, localname = module, name\n",
        "                while \".\" in localname:\n",
        "                    prefix, localname = localname.split(\".\", 1)\n",
        "                    base = base.__getattr__(prefix)\n",
        "                self.name_base_localname.append((name, base, localname))\n",
        "                if \"intrinsic_parameter\" not in name:\n",
        "                    param.requires_grad_(False)\n",
        "        if said:\n",
        "            self.intrinsic_parameter_said = nn.Parameter(\n",
        "                torch.ones((length)).cpu() if device == \"cpu\" else torch.ones((length)).cuda())\n",
        "            module.register_parameter(\n",
        "                \"intrinsic_parameter_said\", self.intrinsic_parameter_said)\n",
        "            setattr(module, \"intrinsic_parameter_said\",\n",
        "                    self.intrinsic_parameter_said)\n",
        "\n",
        "    def get_projection_params(self, DD, device):\n",
        "        if self.projection == \"fastfood\":\n",
        "            return fastfood_vars(DD, device)\n",
        "        elif self.projection == \"random\":\n",
        "            return random_vars(DD, self.intrinsic_dimension, device)\n",
        "\n",
        "    def move_to(self, x_tuple, target):\n",
        "        if isinstance(x_tuple, torch.Tensor):\n",
        "            return x_tuple.to(target)\n",
        "        a = []\n",
        "        for x in x_tuple:\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                a.append(x.to(target))\n",
        "            else:\n",
        "                a.append(x)\n",
        "        return tuple(a)\n",
        "\n",
        "    def requires_to(self, x_tuple, target):\n",
        "        if isinstance(x_tuple, torch.Tensor):\n",
        "            x_tuple.requires_grad_(target)\n",
        "        for x in x_tuple:\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                x.requires_grad_(target)\n",
        "\n",
        "    def projection_vars_requires_grad_(self, requires_grad):\n",
        "        for item in self.projection_params.items():\n",
        "            self.requires_to(item, requires_grad)\n",
        "\n",
        "    def get_projected_param(self, intrinsic_vec, DD, projection_params, init_shape):\n",
        "        if self.projection == \"fastfood\":\n",
        "            return fastfood_torched(intrinsic_vec, DD, projection_params).view(\n",
        "                    init_shape\n",
        "                )\n",
        "        elif self.projection == \"random\":\n",
        "            return random_torched(intrinsic_vec, projection_params).view(\n",
        "                init_shape\n",
        "            )\n",
        "\n",
        "    def __call__(self, module, inputs):\n",
        "        index = 0\n",
        "        with torch.enable_grad():\n",
        "            for name, base, localname in self.name_base_localname:\n",
        "                if localname == \"intrinsic_parameter\":\n",
        "                    continue\n",
        "                if self.device == \"cpu\":\n",
        "                    self.initial_value[name] = self.initial_value[name].to(\n",
        "                        getattr(base, localname))\n",
        "                    device_dtype = getattr(base, localname).dtype\n",
        "\n",
        "                init_shape = self.initial_value[name].size()\n",
        "                DD = np.prod(init_shape)\n",
        "                if self.device == \"cpu\":\n",
        "                    self.projection_params[name] = self.move_to(\n",
        "                        self.projection_params[name], module.intrinsic_parameter.device)\n",
        "\n",
        "                ray = self.get_projected_param(module.intrinsic_parameter, DD, self.projection_params[name], init_shape)\n",
        "                if self.said:\n",
        "                    ray = ray * self.intrinsic_parameter_said[index]\n",
        "                if self.device == \"cpu\":\n",
        "                    param = (self.initial_value[name] + ray).to(device_dtype)\n",
        "                else:\n",
        "                    param = (self.initial_value[name] + ray)\n",
        "                delattr(base, localname)\n",
        "                setattr(base, localname, param)\n",
        "                index += 1\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, intrinsic_dimension, output_dir, str_filter=set(), said=False, projection=\"fastfood\", device=\"cpu\"):\n",
        "        for k, hook in module._forward_pre_hooks.items():\n",
        "            if isinstance(hook, IntrinsicDimensionLight) and hook.name == name:\n",
        "                raise RuntimeError(\"Cannot register two intrinsic dimension hooks on \"\n",
        "                                   \"the same parameter {}\".format(name))\n",
        "        fn = IntrinsicDimensionLight(\n",
        "            module, intrinsic_dimension, output_dir, str_filter, said, projection, device)\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_with_tensor(module, intrinsic_vector, str_filter=set()):\n",
        "        assert isinstance(intrinsic_vector,\n",
        "                          torch.Tensor) and intrinsic_vector.ndim == 1\n",
        "\n",
        "        for k, hook in module._forward_pre_hooks.items():\n",
        "            if isinstance(hook, IntrinsicDimensionLight) and hook.name == name:\n",
        "                raise RuntimeError(\"Cannot register two intrinsic dimension hooks on \"\n",
        "                                   \"the same parameter {}\".format(name))\n",
        "        fn = IntrinsicDimensionLight(\n",
        "            module, intrinsic_vector.size(0), str_filter, False)\n",
        "        fn.intrinsic_parameter = intrinsic_vector\n",
        "        module.register_forward_pre_hook(fn)\n",
        "        return fn\n",
        "\n",
        "\n",
        "def intrinsic_dimension(module, intrinsic_dimension,  output_dir, str_filter, projection, device=\"cpu\"):\n",
        "    IntrinsicDimensionLight.apply(\n",
        "        module, intrinsic_dimension,  output_dir, str_filter, False, projection, device)\n",
        "    return module\n",
        "\n",
        "\n",
        "def intrinsic_dimension_said(module, intrinsic_dimension,  output_dir, str_filter, projection, device=\"cpu\"):\n",
        "    IntrinsicDimensionLight.apply(\n",
        "        module, intrinsic_dimension,  output_dir, str_filter, True, projection, device)\n",
        "    return module\n"
      ],
      "metadata": {
        "id": "6osasJA4mcSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntrinsicSAIDFT(FullFT):\n",
        "    def __init__(self, model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb= False, model = None, processor = None):\n",
        "        super().__init__(model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb, model, processor)\n",
        "        self.method_name = 'IntrinsicSAID_FT'\n",
        "        intrinsic_dim = 20000  # Try different values like 100, 500, 1000\n",
        "        IntrinsicDimensionLight.apply(\n",
        "            module=self.model,\n",
        "            intrinsic_dimension=intrinsic_dim,\n",
        "            output_dir=None,\n",
        "            str_filter=set(),          # You can specify param name patterns to apply this to specific layers\n",
        "            said=True,                # Spatially Adaptive Intrinsic Dim (optional)\n",
        "            projection=\"fastfood\",     # Or \"random\"\n",
        "            device=\"cuda\"               # Or \"cuda\"\n",
        "        )\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        # print(total_params, trainable_params)\n",
        "\n",
        "    def Tune(self):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory containing 'train' and 'val' subdirectories,\n",
        "                            each with 'real' and 'fake' subdirectories\n",
        "            real_folder:\n",
        "            fake_folder:\n",
        "            num_epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            learning_rate (float): Learning rate for optimizer\n",
        "            use_wandb (boolean): Use wandb's logging\n",
        "        \"\"\"\n",
        "\n",
        "        return super().Tune()"
      ],
      "metadata": {
        "id": "Fn0htnVfdcXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/purewater0901/deepfake-detection.git\n",
        "\n",
        "from deepfake_detection.model.dfdet import DeepfakeDetectionModel\n",
        "from deepfake_detection.config import Config"
      ],
      "metadata": {
        "id": "i6UjX3KegiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"weights/model.ckpt\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Downloading model\")\n",
        "    os.makedirs(\"weights\", exist_ok=True)\n",
        "    os.system(f\"wget https://huggingface.co/yermandy/deepfake-detection/resolve/main/model.ckpt -O {model_path}\")\n",
        "ckpt = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "model = DeepfakeDetectionModel(Config(**ckpt[\"hyper_parameters\"]))\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "print(model)\n",
        "\n",
        "# Get preprocessing function\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", use_fast=True)"
      ],
      "metadata": {
        "id": "io17oRl8eAxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params, trainable_params)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(total_params, trainable_params)"
      ],
      "metadata": {
        "id": "0wXv82TgqB1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'model_name': 'vit_large_patch14_clip_224.openai',\n",
        "    \"data_dir\":\"/root/.cache/kagglehub/datasets/katsuyamucb/madde-dataset/versions/13\",\n",
        "    \"real_folder\" : 'Real_split',\n",
        "    \"fake_folder\" : 'All_fakes_split',\n",
        "    \"num_epochs\":10,\n",
        "    \"batch_size\":16,\n",
        "    \"learning_rate\":0.0001,\n",
        "    'use_wandb':False\n",
        "}\n",
        "\n",
        "intrinsic = IntrinsicSAIDFT(model = model, processor = processor, **config)\n",
        "#FullFineTuner.set_TestFolder('StyleGAN_split')\n",
        "tuned_model = intrinsic.Experiment('clip_pretrained')"
      ],
      "metadata": {
        "id": "kYeOdvIfeCOb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-TRGQtMQSTh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}