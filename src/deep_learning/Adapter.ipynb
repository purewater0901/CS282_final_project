{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Codebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/scratch/users/davenlim/learn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loading Libraries\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm  # Import tqdm for progress bars\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "from IPython.display import display\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Wandb\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for deepfake detection\n",
    "class DeepfakeDataset(Dataset):\n",
    "    \"\"\"Custom dataset for testing deepfake detection models with customizable class folders\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        root_dir: str, \n",
    "        real_folder: str = 'Real', \n",
    "        fake_folder: str = 'Fake',\n",
    "        transform=None,\n",
    "        processor=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing class folders\n",
    "            real_folder (str): Name of the folder containing real images\n",
    "            fake_folder (str): Name of the folder containing fake images\n",
    "            transform (callable, optional): Optional transform to be applied on images, necessary if you create models via timm library\n",
    "            processor (callable, optional): Pre-trained processor, necessary if you create models via Transformer library\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        self.class_folders = {\n",
    "            0: real_folder,  # 0 = real\n",
    "            1: fake_folder,  # 1 = fake\n",
    "        }\n",
    "        \n",
    "        self.samples = []\n",
    "        self.load_samples()\n",
    "    \n",
    "    def load_samples(self):\n",
    "        \"\"\"Load all image paths and their corresponding labels\"\"\"\n",
    "        for class_idx, folder_name in self.class_folders.items():\n",
    "            class_dir = os.path.join(self.root_dir, folder_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                raise FileNotFoundError(f\"Directory not found: {class_dir}\")\n",
    "            \n",
    "            # Add all valid images from this class folder\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png')): # We restrict png format, in order to avoid overfitting to the difference in format\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.samples.append((img_path, class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            elif self.processor:\n",
    "                image = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0) \n",
    "                # processor returns dictionary, so reduce dimension here\n",
    "                \n",
    "            return image, label, img_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a placeholder image and the label\n",
    "            placeholder = torch.zeros((3, 299, 299))\n",
    "            return placeholder, label, img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner():\n",
    "    \"\"\"\n",
    "    A Class of fine-tuning.\n",
    "            \n",
    "    Args:\n",
    "        data_dir (str): Directory containing 'train' and 'val' subdirectories, \n",
    "                        each with 'real' and 'fake' subdirectories\n",
    "        real_folder:\n",
    "        fake_folder:\n",
    "        num_epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        use_wandb (boolean): Use wandb's logging \n",
    "        model: Optional model if you load your model from transformer library\n",
    "        processor: Optional processor if you load your model from transformer library - it comes with a pre-trained processor\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb = False, model = None, processor = None):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load the model using timm if the model is None\n",
    "        if model == None:\n",
    "            self.model = timm.create_model(self.model_name, pretrained=True, num_classes=2)\n",
    "            print(\"Loaded \", model_name, \" for fine-tuning\")\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.real_folder = real_folder\n",
    "        self.fake_folder = fake_folder\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_wandb = use_wandb\n",
    "        self.test_fake_folder = None\n",
    "        self.processor = processor\n",
    "\n",
    "    def set_TestFolder(self, test_fake_folder):\n",
    "        self.test_fake_folder = test_fake_folder\n",
    "\n",
    "    def get_Train_Val_loader(self):\n",
    "\n",
    "        if self.processor:\n",
    "            train_dataset = DeepfakeDataset(\n",
    "                root_dir=self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Train'), \n",
    "                fake_folder= os.path.join(self.fake_folder, 'Train'),\n",
    "                processor = self.processor\n",
    "            )\n",
    "        \n",
    "            val_dataset = DeepfakeDataset(\n",
    "                root_dir= self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Validation'), \n",
    "                fake_folder= os.path.join(self.fake_folder, 'Validation'),\n",
    "                processor = self.processor\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # Get the config file from timm\n",
    "            config = resolve_data_config({}, model=self.model)\n",
    "            base_transform = create_transform(**config)\n",
    "            # Data augmentation and normalization for training\n",
    "            train_transform_list = base_transform.transforms\n",
    "            train_transform_list.append(transforms.RandomHorizontalFlip())\n",
    "            train_transform_list.append(transforms.RandomRotation(10))\n",
    "            \n",
    "            brightness = np.random.uniform(0.05, 0.2)  # Random value between 0.05 and 0.2... you can change if you want\n",
    "            contrast = np.random.uniform(0.05, 0.2)\n",
    "            saturation = np.random.uniform(0.05, 0.2)\n",
    "            hue = np.random.uniform(0, 0.1)  # Hue is typically smaller values\n",
    "            train_transform_list.append(transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue = hue))\n",
    "                    \n",
    "            train_transform = transforms.Compose(train_transform_list)\n",
    "            val_transform = base_transform\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = DeepfakeDataset(\n",
    "                root_dir=self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Train'), \n",
    "                fake_folder= os.path.join(self.fake_folder, 'Train'),\n",
    "                transform=train_transform\n",
    "            )\n",
    "            \n",
    "            val_dataset = DeepfakeDataset(\n",
    "                root_dir= self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Validation'), \n",
    "                fake_folder= os.path.join(self.fake_folder, 'Validation'),\n",
    "                transform=val_transform,\n",
    "            )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def get_Test_loader(self, test_folder):\n",
    "        target = test_folder\n",
    "        if self.processor:\n",
    "            # Create dataset\n",
    "            dataset = DeepfakeDataset(\n",
    "                root_dir=self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Test'), \n",
    "                fake_folder= os.path.join(target, 'Test'),\n",
    "                processor = self.processor\n",
    "            )\n",
    "        else:\n",
    "            # Get the config file from timm\n",
    "            config = resolve_data_config({}, model=self.model)\n",
    "            base_transform = create_transform(**config)\n",
    "            \n",
    "            \n",
    "            # Create dataset\n",
    "            dataset = DeepfakeDataset(\n",
    "                root_dir=self.data_dir,\n",
    "                real_folder= os.path.join(self.real_folder, 'Test'), \n",
    "                fake_folder= os.path.join(target, 'Test'),\n",
    "                transform=base_transform\n",
    "            )\n",
    "            \n",
    "        # Create data loader\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return data_loader\n",
    "\n",
    "    def Tune(self):\n",
    "        # function to override\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def Evaluation(self, test_folder):\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Get dataloader\n",
    "        test_loader = self.get_Test_loader(test_folder)\n",
    "        print('\\n\\n----- Test on ',test_folder,'-----')\n",
    "        print(f\"Device: {device}\")\n",
    "        \n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        model = self.model.to(device)\n",
    "        \n",
    "        # Lists to store results\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        all_paths = []\n",
    "        confidence_threshold = 0.5\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, paths in tqdm(test_loader, desc=\"Testing\"):\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Convert outputs to probabilities\n",
    "                probs = torch.softmax(outputs.float(), dim=1).cpu().numpy() # BF16 to float()\n",
    "                \n",
    "                # Convert to binary predictions using threshold\n",
    "                preds = (probs[:,1] >= confidence_threshold).astype(int)\n",
    "                \n",
    "                # Store results\n",
    "                all_preds.extend(preds)\n",
    "                all_probs.extend(probs)\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_paths.extend(paths)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_probs = np.array(all_probs)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(all_labels, all_preds, \n",
    "                                      target_names=['Real', 'Fake'], \n",
    "                                      output_dict=True)\n",
    "\n",
    "        test_acc = (all_preds == all_labels).mean()\n",
    "\n",
    "        print(f\"\\n\\n{'-'*50}\")\n",
    "        print(f\"Test Result Summary:\")\n",
    "        print(f\"{'-'*50}\")\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # Visualize the result ------------------------------------------------\n",
    "        report_df = pd.DataFrame(report)\n",
    "        display(report_df)\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, ['Real', 'Fake'], rotation=45)\n",
    "        plt.yticks(tick_marks, ['Real', 'Fake'])\n",
    "        \n",
    "        # Add text annotations to confusion matrix\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Logging\n",
    "        if self.use_wandb:\n",
    "            \n",
    "            all_probs = np.array(all_probs)\n",
    "            fpr, tpr, _ = roc_curve(all_labels, all_probs[:,1])\n",
    "            wandb.log({\n",
    "                \"test_dataset\": test_folder,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                # Confusion Matrix\n",
    "                \"test_confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=all_labels,\n",
    "                    preds=all_preds,\n",
    "                    class_names=['Real', 'Fake']\n",
    "                ),\n",
    "                # ROC curve\n",
    "                \"test_roc_curve\": wandb.plot.roc_curve(\n",
    "                    y_true= all_labels,\n",
    "                    y_probas= all_probs,\n",
    "                    labels=['Real', 'Fake']\n",
    "                )\n",
    "            })\n",
    "            \n",
    "        return report_df\n",
    "\n",
    "    def log_wandb_train(\n",
    "            self,\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            all_probs,\n",
    "            epoch,\n",
    "            train_loss,\n",
    "            train_acc,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "            optimizer,\n",
    "        ):\n",
    "        if self.use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "                    \n",
    "            # Extra logging for the last epoch\n",
    "            if epoch == self.num_epochs - 1 and self.use_wandb:\n",
    "                # Confusion Matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                wandb.log({\n",
    "                    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                        probs=None,\n",
    "                        y_true=all_labels,\n",
    "                        preds=all_preds,\n",
    "                        class_names=['Real', 'Fake']\n",
    "                    )\n",
    "                })\n",
    "    \n",
    "                # ROC curve - use probs instead of preds\n",
    "                all_probs = np.array(all_probs)\n",
    "                fpr, tpr, _ = roc_curve(all_labels, all_probs[:,1])\n",
    "                wandb.log({\n",
    "                    \"roc_curve\": wandb.plot.roc_curve(\n",
    "                        y_true= all_labels,\n",
    "                        y_probas= all_probs,\n",
    "                        labels=['Real', 'Fake']\n",
    "                    )\n",
    "                })\n",
    "        \n",
    "    def Experiment(self, wandb_run_name):\n",
    "        # Initilize Wandb\n",
    "        if self.use_wandb:\n",
    "            wandb.init(project='Fine-Tuning Experiment', name=wandb_run_name)\n",
    "        \n",
    "            # Log the experimental setting\n",
    "            wandb.config.update({\n",
    "                \"model\": self.model_name,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"num_epochs\": self.num_epochs,\n",
    "                \"fine_tuning_type\": \"full\",\n",
    "                \"dataset_dir\": self.data_dir,\n",
    "                \"real_folder\": self.real_folder,\n",
    "                \"fake_folder\": self.fake_folder\n",
    "            })\n",
    "    \n",
    "            total_params = sum(p.numel() for p in self.model.parameters())\n",
    "            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            wandb.log({\n",
    "                \"total_parameters\": total_params,\n",
    "                \"trainable_parameters\": trainable_params,\n",
    "                \"frozen_parameters\": total_params - trainable_params,\n",
    "                \"percent_trainable\": 100 * trainable_params / total_params\n",
    "            })\n",
    "            \n",
    "            # Log the model itself, (don't know if we need this or not)\n",
    "            # wandb.watch(self.model, log=\"all\", log_freq=100)\n",
    "    \n",
    "        # Fine-tune the model\n",
    "        tuned_model = self.Tune()\n",
    "        report_df_seen = self.Evaluation(self.fake_folder)\n",
    "        if self.test_fake_folder != None:\n",
    "            report_df_unseen = self.Evaluation(self.test_fake_folder)\n",
    "    \n",
    "        if self.use_wandb:\n",
    "            model_artifact = wandb.Artifact(\n",
    "                name=f\"{self.method_name}-{self.model_name}\", \n",
    "                type=\"model\"\n",
    "            )\n",
    "            #model_artifact.add_file(model_save_path)\n",
    "            wandb.log_artifact(model_artifact)\n",
    "            wandb.finish()\n",
    "\n",
    "        return tuned_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullFT(FineTuner):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        data_dir,\n",
    "        real_folder,\n",
    "        fake_folder,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        use_wandb= False,\n",
    "        model = None,\n",
    "        processor = None):\n",
    "        super().__init__(model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb, model, processor)\n",
    "        self.method_name = 'Full_FT' # Just name your method for the record\n",
    "\n",
    "    def Tune(self):\n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "    \n",
    "        # Load the model\n",
    "        #model = timm.create_model(self.model_name, pretrained=True, num_classes=2)\n",
    "        self.model = self.model.to(device)\n",
    "        model_save_path= f\"{self.model_name}.pth\"\n",
    "                   \n",
    "        # Get data loader for training and validation\n",
    "        train_loader, val_loader = self.get_Train_Val_loader()\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        # Create a tqdm progress bar for epochs\n",
    "        epoch_loop = tqdm(range(self.num_epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "        for epoch in epoch_loop:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            for inputs, labels, pathes in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs).float()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            train_acc = correct / total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels, pathes in val_loader: #val_loop:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_running_loss += loss.item() * inputs.size(0)\n",
    "                    probs = torch.nn.functional.softmax(outputs.float(), dim=1)\n",
    "                    _, predicted = torch.max(outputs.float(), 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            val_loss = val_running_loss / len(val_loader.dataset)\n",
    "            val_acc = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduler step\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            print(f\"\\n\\n{'-'*50}\")\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} Summary:\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            # Save the best model\n",
    "            if val_acc >= best_val_acc:\n",
    "                torch.save(self.model.state_dict(), model_save_path)\n",
    "                print(f\"✅ New best model saved! Validation accuracy: {val_acc:.4f} (previous best: {best_val_acc:.4f})\")\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            # Also save a checkpoint every 5 epoch\n",
    "            if (epoch + 1) % 5 == 0: \n",
    "                checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'best_val_acc': best_val_acc\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            print(f\"Training:   Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f} ({correct}/{total})\")\n",
    "            print(f\"Validation: Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} ({val_correct}/{val_total})\")\n",
    "            \n",
    "            # Calculate and display improvement or regression\n",
    "            if epoch > 0:\n",
    "                train_loss_change = train_loss - train_losses[-2]\n",
    "                train_acc_change = train_acc - train_accuracies[-2]\n",
    "                val_loss_change = val_loss - val_losses[-2]\n",
    "                val_acc_change = val_acc - val_accuracies[-2]\n",
    "                \n",
    "                print(f\"Changes from previous epoch:\")\n",
    "                print(f\"  Train Loss: {train_loss_change:+.4f} | Train Acc: {train_acc_change:+.4f}\")\n",
    "                print(f\"  Val Loss: {val_loss_change:+.4f} | Val Acc: {val_acc_change:+.4f}\")\n",
    "            \n",
    "            # Display current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "            #Load the best model\n",
    "            print(f\"Loading best model from {model_save_path}\")\n",
    "            self.model.load_state_dict(torch.load(model_save_path))\n",
    "            \n",
    "            # Log in wandb\n",
    "            self.log_wandb_train(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                all_probs,\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                val_loss,\n",
    "                val_acc,\n",
    "                optimizer\n",
    "            )\n",
    "        \n",
    "        # Final summary at the end of training\n",
    "        tqdm.write(f\"\\nTraining completed after {self.num_epochs} epochs\")\n",
    "        tqdm.write(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterTuner(FineTuner):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        data_dir,\n",
    "        real_folder,\n",
    "        fake_folder,\n",
    "        num_epochs,\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        use_wandb= False,\n",
    "        model = None,\n",
    "        processor = None):\n",
    "        super().__init__(model_name, data_dir, real_folder, fake_folder, num_epochs, batch_size, learning_rate, use_wandb, model, processor)\n",
    "        self.method_name = 'Adapter'\n",
    "        \n",
    "        # self.pre_adapter = PreAdapter()\n",
    "        # self.pre_adapter.apply(self.init_weights)\n",
    "\n",
    "    # def init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):\n",
    "    #         nn.init.xavier_uniform_(m.weight)\n",
    "    #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def Tune(self):\n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "    \n",
    "        # Load the model\n",
    "        #model = timm.create_model(self.model_name, pretrained=True, num_classes=2)\n",
    "        self.model = self.model.to(device)\n",
    "        # self.pre_adapter = self.pre_adapter.to(device)\n",
    "        model_save_path= f\"{self.model_name}_Adapter.pth\"\n",
    "                   \n",
    "        # Get data loader for training and validation\n",
    "        train_loader, val_loader = self.get_Train_Val_loader()\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # # Define the optimizer to only update parameters of adapters\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # for n, p in model.named_parameters():\n",
    "        #     if \"adapter\" in n:\n",
    "        #         p.requires_grad = True\n",
    "            \n",
    "        optimizer = optim.Adam(\n",
    "            [p for n, p in model.named_parameters() if p.requires_grad],\n",
    "            lr=self.learning_rate\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        best_val_acc = 0.0\n",
    "        \n",
    "        # Create a tqdm progress bar for epochs\n",
    "        epoch_loop = tqdm(range(self.num_epochs), desc=\"Training Progress\", unit=\"epoch\")\n",
    "        for epoch in epoch_loop:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    \n",
    "            for inputs, labels, pathes in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                # inputs = processor(images=inputs, return_tensors=\"pt\").to(device)\n",
    "                # pixel_values = processor(images=inputs, return_tensors=\"pt\").to(device)[\"pixel_values\"]\n",
    "                # inputs = {\"pixel_values\": inputs}\n",
    "                \n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                # added pre_adapter\n",
    "                # inputs = self.pre_adapter(inputs)\n",
    "                # outputs = self.model(inputs).float()\n",
    "                # outputs = self.model(pixel_values=pixel_values).logits\n",
    "                # outputs = self.model(**inputs).logits\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                # running_loss += loss.item() * inputs[\"pixel_values\"].size(0)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "            train_acc = correct / total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            all_probs = []\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels, pathes in val_loader: #val_loop:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    # inputs = processor(images=inputs, return_tensors=\"pt\").to(device)\n",
    "                    # pixel_values = processor(images=inputs, return_tensors=\"pt\").to(device)[\"pixel_values\"]\n",
    "                    # inputs = {\"pixel_values\": inputs}\n",
    "\n",
    "                    # added pre_adapter\n",
    "                    # inputs = self.pre_adapter(inputs)\n",
    "                    # outputs = self.model(inputs)\n",
    "                    # outputs = self.model(pixel_values=pixel_values).logits\n",
    "                    # outputs = self.model(**inputs).logits\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # val_running_loss += loss.item() * inputs[\"pixel_values\"].size(0)\n",
    "                    val_running_loss += loss.item() * inputs.size(0)\n",
    "                    probs = torch.nn.functional.softmax(outputs.float(), dim=1)\n",
    "                    _, predicted = torch.max(outputs.float(), 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            val_loss = val_running_loss / len(val_loader.dataset)\n",
    "            val_acc = val_correct / val_total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduler step\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            print(f\"\\n\\n{'-'*50}\")\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs} Summary:\")\n",
    "            print(f\"{'-'*50}\")\n",
    "            # Save the best model\n",
    "            if val_acc >= best_val_acc:\n",
    "                torch.save(self.model.state_dict(), model_save_path)\n",
    "                print(f\"✅ New best model saved! Validation accuracy: {val_acc:.4f} (previous best: {best_val_acc:.4f})\")\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            # Also save a checkpoint every 5 epoch\n",
    "            if (epoch + 1) % 5 == 0: \n",
    "                checkpoint_path = f\"checkpoint_epoch_{epoch+1}_Adapter.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'best_val_acc': best_val_acc\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "            \n",
    "            print(f\"Training:   Loss: {train_loss:.4f} | Accuracy: {train_acc:.4f} ({correct}/{total})\")\n",
    "            print(f\"Validation: Loss: {val_loss:.4f} | Accuracy: {val_acc:.4f} ({val_correct}/{val_total})\")\n",
    "            \n",
    "            # Calculate and display improvement or regression\n",
    "            if epoch > 0:\n",
    "                train_loss_change = train_loss - train_losses[-2]\n",
    "                train_acc_change = train_acc - train_accuracies[-2]\n",
    "                val_loss_change = val_loss - val_losses[-2]\n",
    "                val_acc_change = val_acc - val_accuracies[-2]\n",
    "                \n",
    "                print(f\"Changes from previous epoch:\")\n",
    "                print(f\"  Train Loss: {train_loss_change:+.4f} | Train Acc: {train_acc_change:+.4f}\")\n",
    "                print(f\"  Val Loss: {val_loss_change:+.4f} | Val Acc: {val_acc_change:+.4f}\")\n",
    "            \n",
    "            # Display current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "            #Load the best model\n",
    "            print(f\"Loading best model from {model_save_path}\")\n",
    "            self.model.load_state_dict(torch.load(model_save_path))\n",
    "            \n",
    "            # Log in wandb\n",
    "            self.log_wandb_train(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                all_probs,\n",
    "                epoch,\n",
    "                train_loss,\n",
    "                train_acc,\n",
    "                val_loss,\n",
    "                val_acc,\n",
    "                optimizer\n",
    "            )\n",
    "        \n",
    "        # Final summary at the end of training\n",
    "        tqdm.write(f\"\\nTraining completed after {self.num_epochs} epochs\")\n",
    "        tqdm.write(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from transformers import CLIPImageProcessor\n",
    "from transformers import CLIPVisionModel\n",
    "\n",
    "from deepfake_detection.model.dfdet import DeepfakeDetectionModel\n",
    "from deepfake_detection.config import Config\n",
    "# from transformers.adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to load pre-deepfake trained model\n",
    "model_path = \"./weights/model.ckpt\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading model\")\n",
    "    os.makedirs(\"weights\", exist_ok=True)\n",
    "    os.system(f\"wget https://huggingface.co/yermandy/deepfake-detection/resolve/main/model.ckpt -O {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearProbe(\n",
      "  (linear): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "Linear(in_features=1024, out_features=2, bias=True)\n",
      "LinearProbe(\n",
      "  (linear): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "base_model = DeepfakeDetectionModel(Config(**ckpt[\"hyper_parameters\"]))\n",
    "base_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "print(base_model.model)\n",
    "\n",
    "# Save the classifier layer\n",
    "classifier = base_model.model.linear\n",
    "print(classifier)\n",
    "\n",
    "# Remove it from the model so it's not run inside forward()\n",
    "base_model.model.linear = nn.Identity()\n",
    "\n",
    "print(base_model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter.0.weight\n",
      "adapter.0.bias\n",
      "adapter.3.weight\n",
      "adapter.3.bias\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/20 [00:00<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 1/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.8683 (previous best: 0.0000)\n",
      "Training:   Loss: 0.3271 | Accuracy: 0.8573 (2193/2558)\n",
      "Validation: Loss: 0.2770 | Accuracy: 0.8683 (277/319)\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 1/20 [00:37<11:55, 37.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 2/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9060 (previous best: 0.8683)\n",
      "Training:   Loss: 0.1378 | Accuracy: 0.9457 (2419/2558)\n",
      "Validation: Loss: 0.2926 | Accuracy: 0.9060 (289/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.1892 | Train Acc: +0.0884\n",
      "  Val Loss: +0.0156 | Val Acc: +0.0376\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 2/20 [01:15<11:18, 37.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 3/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0847 | Accuracy: 0.9676 (2475/2558)\n",
      "Validation: Loss: 0.3072 | Accuracy: 0.8997 (287/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0531 | Train Acc: +0.0219\n",
      "  Val Loss: +0.0146 | Val Acc: -0.0063\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 3/20 [01:51<10:30, 37.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 4/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9154 (previous best: 0.9060)\n",
      "Training:   Loss: 0.0830 | Accuracy: 0.9629 (2463/2558)\n",
      "Validation: Loss: 0.2571 | Accuracy: 0.9154 (292/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0018 | Train Acc: -0.0047\n",
      "  Val Loss: -0.0501 | Val Acc: +0.0157\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 4/20 [02:31<10:07, 37.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 5/20 Summary:\n",
      "--------------------------------------------------\n",
      "Checkpoint saved: checkpoint_epoch_5_Adapter.pth\n",
      "Training:   Loss: 0.0513 | Accuracy: 0.9808 (2509/2558)\n",
      "Validation: Loss: 0.3943 | Accuracy: 0.9122 (291/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0317 | Train Acc: +0.0180\n",
      "  Val Loss: +0.1372 | Val Acc: -0.0031\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 5/20 [03:09<09:31, 38.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 6/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9185 (previous best: 0.9154)\n",
      "Training:   Loss: 0.0418 | Accuracy: 0.9848 (2519/2558)\n",
      "Validation: Loss: 0.3087 | Accuracy: 0.9185 (293/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0095 | Train Acc: +0.0039\n",
      "  Val Loss: -0.0855 | Val Acc: +0.0063\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 6/20 [03:47<08:51, 37.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 7/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9248 (previous best: 0.9185)\n",
      "Training:   Loss: 0.0164 | Accuracy: 0.9949 (2545/2558)\n",
      "Validation: Loss: 0.3314 | Accuracy: 0.9248 (295/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0254 | Train Acc: +0.0102\n",
      "  Val Loss: +0.0227 | Val Acc: +0.0063\n",
      "Current learning rate: 0.000500\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 7/20 [04:24<08:12, 37.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 8/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0230 | Accuracy: 0.9922 (2538/2558)\n",
      "Validation: Loss: 0.3863 | Accuracy: 0.9216 (294/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: +0.0066 | Train Acc: -0.0027\n",
      "  Val Loss: +0.0549 | Val Acc: -0.0031\n",
      "Current learning rate: 0.000250\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 8/20 [05:01<07:29, 37.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 9/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0105 | Accuracy: 0.9977 (2552/2558)\n",
      "Validation: Loss: 0.3430 | Accuracy: 0.9154 (292/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0125 | Train Acc: +0.0055\n",
      "  Val Loss: -0.0433 | Val Acc: -0.0063\n",
      "Current learning rate: 0.000250\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 9/20 [05:37<06:47, 37.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 10/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9248 (previous best: 0.9248)\n",
      "Checkpoint saved: checkpoint_epoch_10_Adapter.pth\n",
      "Training:   Loss: 0.0090 | Accuracy: 0.9980 (2553/2558)\n",
      "Validation: Loss: 0.3312 | Accuracy: 0.9248 (295/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0015 | Train Acc: +0.0004\n",
      "  Val Loss: -0.0118 | Val Acc: +0.0094\n",
      "Current learning rate: 0.000250\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 10/20 [06:17<06:18, 37.83s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 11/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0019 | Accuracy: 0.9996 (2557/2558)\n",
      "Validation: Loss: 0.3269 | Accuracy: 0.9216 (294/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0071 | Train Acc: +0.0016\n",
      "  Val Loss: -0.0043 | Val Acc: -0.0031\n",
      "Current learning rate: 0.000250\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 11/20 [06:55<05:40, 37.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 12/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9248 (previous best: 0.9248)\n",
      "Training:   Loss: 0.0016 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3250 | Accuracy: 0.9248 (295/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0003 | Train Acc: +0.0004\n",
      "  Val Loss: -0.0019 | Val Acc: +0.0031\n",
      "Current learning rate: 0.000125\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 12/20 [07:32<05:02, 37.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 13/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9310 (previous best: 0.9248)\n",
      "Training:   Loss: 0.0007 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3227 | Accuracy: 0.9310 (297/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0009 | Train Acc: +0.0000\n",
      "  Val Loss: -0.0022 | Val Acc: +0.0063\n",
      "Current learning rate: 0.000125\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 13/20 [08:11<04:25, 37.97s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 14/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9342 (previous best: 0.9310)\n",
      "Training:   Loss: 0.0005 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3283 | Accuracy: 0.9342 (298/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0001 | Train Acc: +0.0000\n",
      "  Val Loss: +0.0055 | Val Acc: +0.0031\n",
      "Current learning rate: 0.000125\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 14/20 [08:48<03:47, 37.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 15/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9342 (previous best: 0.9342)\n",
      "Checkpoint saved: checkpoint_epoch_15_Adapter.pth\n",
      "Training:   Loss: 0.0005 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3320 | Accuracy: 0.9342 (298/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0000 | Train Acc: +0.0000\n",
      "  Val Loss: +0.0037 | Val Acc: +0.0000\n",
      "Current learning rate: 0.000125\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 15/20 [09:29<03:14, 38.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 16/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9342 (previous best: 0.9342)\n",
      "Training:   Loss: 0.0004 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3359 | Accuracy: 0.9342 (298/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0001 | Train Acc: +0.0000\n",
      "  Val Loss: +0.0039 | Val Acc: +0.0000\n",
      "Current learning rate: 0.000063\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 16/20 [10:07<02:34, 38.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 17/20 Summary:\n",
      "--------------------------------------------------\n",
      "✅ New best model saved! Validation accuracy: 0.9342 (previous best: 0.9342)\n",
      "Training:   Loss: 0.0004 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3372 | Accuracy: 0.9342 (298/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0001 | Train Acc: +0.0000\n",
      "  Val Loss: +0.0013 | Val Acc: +0.0000\n",
      "Current learning rate: 0.000063\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 17/20 [10:45<01:55, 38.35s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 18/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0003 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3392 | Accuracy: 0.9310 (297/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0000 | Train Acc: +0.0000\n",
      "  Val Loss: +0.0020 | Val Acc: -0.0031\n",
      "Current learning rate: 0.000063\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 18/20 [11:21<01:15, 37.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 19/20 Summary:\n",
      "--------------------------------------------------\n",
      "Training:   Loss: 0.0004 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3386 | Accuracy: 0.9310 (297/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: +0.0001 | Train Acc: +0.0000\n",
      "  Val Loss: -0.0006 | Val Acc: +0.0000\n",
      "Current learning rate: 0.000063\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 19/20 [11:58<00:37, 37.31s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 20/20 Summary:\n",
      "--------------------------------------------------\n",
      "Checkpoint saved: checkpoint_epoch_20_Adapter.pth\n",
      "Training:   Loss: 0.0003 | Accuracy: 1.0000 (2558/2558)\n",
      "Validation: Loss: 0.3385 | Accuracy: 0.9310 (297/319)\n",
      "Changes from previous epoch:\n",
      "  Train Loss: -0.0000 | Train Acc: +0.0000\n",
      "  Val Loss: -0.0000 | Val Acc: +0.0000\n",
      "Current learning rate: 0.000031\n",
      "Loading best model from DeepfakeDetectionModel_Adapters_2048_Adapter.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 20/20 [12:35<00:00, 37.80s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 20 epochs\n",
      "Best validation accuracy: 0.9342\n",
      "\n",
      "\n",
      "----- Test on  All_fakes_split -----\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 21/21 [00:06<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Test Result Summary:\n",
      "--------------------------------------------------\n",
      "Test accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Fake</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.926380</td>\n",
       "      <td>0.943038</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.934709</td>\n",
       "      <td>0.934735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.943750</td>\n",
       "      <td>0.925466</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.934608</td>\n",
       "      <td>0.934579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.934985</td>\n",
       "      <td>0.934169</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.934577</td>\n",
       "      <td>0.934576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>321.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Real        Fake  accuracy   macro avg  weighted avg\n",
       "precision    0.926380    0.943038  0.934579    0.934709      0.934735\n",
       "recall       0.943750    0.925466  0.934579    0.934608      0.934579\n",
       "f1-score     0.934985    0.934169  0.934579    0.934577      0.934576\n",
       "support    160.000000  161.000000  0.934579  321.000000    321.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AdapterWrapper(nn.Module):\n",
    "    def __init__(self, base_model, classifier, hidden_dim=1024, bottleneck=1024*2):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bottleneck),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(bottleneck, hidden_dim)\n",
    "        )\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)         # HeadOutput object\n",
    "        x_tensor = features.features          \n",
    "        adapted = x_tensor + self.adapter(x_tensor)  # residual connection\n",
    "        logits = self.classifier(adapted)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = AdapterWrapper(base_model, classifier)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", cache_dir='./huggingface')\n",
    "\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                print(n)\n",
    "\n",
    "config = {\n",
    "    'model_name': 'DeepfakeDetectionModel_Adapters_2048',\n",
    "    \"data_dir\":\"./\",\n",
    "    \"real_folder\" : 'Real_split',\n",
    "    \"fake_folder\" : 'All_fakes_split',\n",
    "    \"num_epochs\":20,\n",
    "    \"batch_size\":16,\n",
    "    \"learning_rate\":0.0005,\n",
    "    'use_wandb':False\n",
    "}\n",
    "\n",
    "adapter_tuner = AdapterTuner(model=model, processor=processor, **config)\n",
    "tuned_model = adapter_tuner.Experiment('adapter_experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model from Hugging Face\n",
    "# model = AutoAdapterModel.from_pretrained(\"openai/clip-vit-large-patch14\", cache_dir='./huggingface')\n",
    "# model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\", cache_dir='./huggingface')\n",
    "# model = AutoAdapterModel.from_pretrained(\"google/vit-base-patch16-224\", cache_dir='./huggingface', ignore_mismatched_sizes=True)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", cache_dir='./huggingface')\n",
    "# processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", cache_dir='./huggingface')\n",
    "\n",
    "# model = AutoAdapterModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# model.add_adapter(\"deepfake\", config=\"pfeiffer\")\n",
    "# model.train_adapter(\"deepfake\")\n",
    "\n",
    "model.add_adapter(\"deepfake\", config=\"pfeiffer\")\n",
    "# Replace classification head for binary (real/fake)\n",
    "# model.add_classification_head(\"deepfake\", num_labels=2)\n",
    "model.train_adapter(\"deepfake\")\n",
    "\n",
    "# model.set_active_adapters(\"deepfake\")\n",
    "# adapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-imdb\", with_head=False, cache_dir='./huggingface')\n",
    "# model.active_adapters = adapter_name\n",
    "\n",
    "# model.set_active_adapters(adapter_name)\n",
    "# # Optionally freeze base model weights except adapter\n",
    "# model.train_adapter(adapter_name)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6881580,
     "sourceId": 11294565,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
